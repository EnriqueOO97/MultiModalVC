{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "6def8739",
            "metadata": {},
            "source": [
                "# Count all the german files in the dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "5f7bcd68",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total rows: 1092009\n",
                        "German ('de') files found: 89700\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Define the path to your CSV file\n",
                "csv_file_path = \"/ceph/shared/ALL/datasets/voxceleb2-V2/voxceleb2-language-identification.csv\"\n",
                "\n",
                "# Read the CSV file\n",
                "df = pd.read_csv(csv_file_path)\n",
                "\n",
                "# Count rows where detected_language is 'de'\n",
                "german_count = df[df['detected_language'] == 'de'].shape[0]\n",
                "\n",
                "print(f\"Total rows: {len(df)}\")\n",
                "print(f\"German ('de') files found: {german_count}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "11c71e64",
            "metadata": {},
            "source": [
                "# generate a csv file of german paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "485e0ed3",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Saved 89700 German entries to: /ceph/shared/ALL/datasets/voxceleb2-V2/voxceleb2-german-only-whisperLarge.csv\n"
                    ]
                }
            ],
            "source": [
                "# ...existing code...\n",
                "# Filter the DataFrame to get only German rows\n",
                "german_df = df[df['detected_language'] == 'de']\n",
                "\n",
                "# Define the output path for the new CSV\n",
                "output_csv_path = \"/ceph/shared/ALL/datasets/voxceleb2-V2/voxceleb2-german-only-whisperLarge.csv\"\n",
                "\n",
                "# Save to CSV (index=False prevents writing the row numbers)\n",
                "german_df.to_csv(output_csv_path, index=False)\n",
                "\n",
                "print(f\"Saved {len(german_df)} German entries to: {output_csv_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d3317e53",
            "metadata": {},
            "source": [
                "# extract the german pairs of audio and video"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "530fdb60",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Reading CSV: /ceph/shared/ALL/datasets/voxceleb2-V2/voxceleb2-german-only-whisperLarge.csv\n",
                        "Found 89700 files to copy.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Copying Files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89700/89700 [43:31<00:00, 34.34it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Copy process completed.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import shutil\n",
                "import os\n",
                "from tqdm import tqdm\n",
                "\n",
                "# --- Configuration ---\n",
                "csv_file_path = \"/ceph/shared/ALL/datasets/voxceleb2-V2/voxceleb2-german-only-whisperLarge.csv\"\n",
                "\n",
                "# Source Roots\n",
                "src_aac_root = \"/ceph/shared/ALL/datasets/voxceleb2-V2/dev/aac\"\n",
                "src_mp4_root = \"/ceph/shared/ALL/datasets/voxceleb2-V2/dev/mp4\"\n",
                "\n",
                "# Destination Roots\n",
                "dst_aac_root = \"/ceph/shared/ALL/datasets/voxceleb2-V2/VoxCeleb2-German/dev/aac\"\n",
                "dst_mp4_root = \"/ceph/shared/ALL/datasets/voxceleb2-V2/VoxCeleb2-German/dev/mp4\"\n",
                "\n",
                "def copy_german_files(csv_path, src_aac, src_mp4, dst_aac, dst_mp4):\n",
                "    print(f\"Reading CSV: {csv_path}\")\n",
                "    df = pd.read_csv(csv_path)\n",
                "    \n",
                "    # We iterate through each row in the CSV\n",
                "    # Each row contains a full path like: /ceph/.../dev/aac/id00015/vUAbwL9omyM/00452.m4a\n",
                "    \n",
                "    print(f\"Found {len(df)} files to copy.\")\n",
                "    \n",
                "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Copying Files\"):\n",
                "        full_audio_path = row['file_path']\n",
                "        \n",
                "        # 1. Parse the path to extract ID, Sequence, and Filename\n",
                "        # We assume the structure ends with: .../aac/<id>/<seq>/<filename>\n",
                "        parts = full_audio_path.split('/')\n",
                "        \n",
                "        filename = parts[-1]       # 00452.m4a\n",
                "        seq_folder = parts[-2]     # vUAbwL9omyM\n",
                "        id_folder = parts[-3]      # id00015\n",
                "        \n",
                "        # 2. Construct Source Paths\n",
                "        # Audio is already known\n",
                "        src_audio = full_audio_path\n",
                "        \n",
                "        # Video path: replace 'aac' with 'mp4' and extension .m4a with .mp4\n",
                "        # We construct it manually to be safe\n",
                "        video_filename = filename.replace('.m4a', '.mp4')\n",
                "        src_video = os.path.join(src_mp4, id_folder, seq_folder, video_filename)\n",
                "        \n",
                "        # 3. Construct Destination Paths (preserving structure)\n",
                "        # Audio Destination\n",
                "        dst_audio_dir = os.path.join(dst_aac, id_folder, seq_folder)\n",
                "        dst_audio_file = os.path.join(dst_audio_dir, filename)\n",
                "        \n",
                "        # Video Destination\n",
                "        dst_video_dir = os.path.join(dst_mp4, id_folder, seq_folder)\n",
                "        dst_video_file = os.path.join(dst_video_dir, video_filename)\n",
                "        \n",
                "        # 4. Create Directories\n",
                "        os.makedirs(dst_audio_dir, exist_ok=True)\n",
                "        os.makedirs(dst_video_dir, exist_ok=True)\n",
                "        \n",
                "        # 5. Copy Files\n",
                "        # Copy Audio\n",
                "        if os.path.exists(src_audio):\n",
                "            if not os.path.exists(dst_audio_file):\n",
                "                try:\n",
                "                    shutil.copy2(src_audio, dst_audio_file)\n",
                "                except Exception as e:\n",
                "                    print(f\"Error copying audio {src_audio}: {e}\")\n",
                "        else:\n",
                "            # print(f\"Warning: Source audio missing: {src_audio}\")\n",
                "            pass\n",
                "\n",
                "        # Copy Video\n",
                "        if os.path.exists(src_video):\n",
                "            if not os.path.exists(dst_video_file):\n",
                "                try:\n",
                "                    shutil.copy2(src_video, dst_video_file)\n",
                "                except Exception as e:\n",
                "                    print(f\"Error copying video {src_video}: {e}\")\n",
                "        else:\n",
                "            # print(f\"Warning: Source video missing: {src_video}\")\n",
                "            pass\n",
                "\n",
                "    print(\"Copy process completed.\")\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    copy_german_files(csv_file_path, src_aac_root, src_mp4_root, dst_aac_root, dst_mp4_root)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "14987937",
            "metadata": {},
            "outputs": [
                {
                    "ename": "FileNotFoundError",
                    "evalue": "[Errno 2] No such file or directory: '/ceph/shared/ALL/datasets/voxceleb2-V2/vox2_german_video_seg16s'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# --- Processing ---\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Get the list of items in both directories\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# We use set() to make mathematical subtraction easy\u001b[39;00m\n\u001b[1;32m     14\u001b[0m ref_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(ref_path))\n\u001b[0;32m---> 15\u001b[0m target_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Calculate the difference\u001b[39;00m\n\u001b[1;32m     18\u001b[0m missing_items \u001b[38;5;241m=\u001b[39m ref_items \u001b[38;5;241m-\u001b[39m target_items\n",
                        "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/ceph/shared/ALL/datasets/voxceleb2-V2/vox2_german_video_seg16s'"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "\n",
                "# --- Configuration ---\n",
                "# The reference folder (the one that has all 714 items)\n",
                "ref_path = \"/ceph/shared/ALL/datasets/voxceleb2-V2/VoxCeleb2-German/dev/aac\"\n",
                "\n",
                "# The target folder (the one with 711 items)\n",
                "# UPDATE THIS PATH to the actual location of 'vox2_german_video_seg16s'\n",
                "target_path = \"/ceph/shared/ALL/datasets/voxceleb2-V2/VoxCeleb2-German/dev/processedVideos/vox2_german/vox2_german_video_seg16s\" \n",
                "\n",
                "# --- Processing ---\n",
                "# Get the list of items in both directories\n",
                "# We use set() to make mathematical subtraction easy\n",
                "ref_items = set(os.listdir(ref_path))\n",
                "target_items = set(os.listdir(target_path))\n",
                "\n",
                "# Calculate the difference\n",
                "missing_items = ref_items - target_items\n",
                "\n",
                "print(f\"Items in Reference (aac): {len(ref_items)}\")\n",
                "print(f\"Items in Target (seg16s): {len(target_items)}\")\n",
                "print(f\"Number of missing items: {len(missing_items)}\")\n",
                "print(\"-\" * 30)\n",
                "\n",
                "if len(missing_items) > 0:\n",
                "    print(\"The following folders are missing in 'vox2_german_video_seg16s':\")\n",
                "    for item in sorted(missing_items):\n",
                "        print(item)\n",
                "else:\n",
                "    print(\"No missing items found! The folders match.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "18e8a410",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "GPU is available: NVIDIA RTX PRO 6000 Blackwell Server Edition MIG 4g.96gb\n",
                        "mms-llama-blk-l\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import os\n",
                "# Check if CUDA is available\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
                "else:\n",
                "    print(\"GPU is not available.\")\n",
                "\n",
                "# Returns the name of the Conda environment\n",
                "print(os.environ.get('CONDA_DEFAULT_ENV'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "29fb71b3",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/ceph/home/TUG/olivares-tug/.conda/envs/mms-llama-blk-l/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
                        "  WeightNorm.apply(module, name, dim)\n",
                        "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d1344da88fd0460d93f66d0baee7ae51",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "trainable params: 9,175,040 || all params: 3,221,924,864 || trainable%: 0.2848\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
                        "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
                        "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
                        "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
                        "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "95f0a1b2c9f5419989c8ed2c65410714",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "trainable params: 9,175,040 || all params: 3,221,924,864 || trainable%: 0.2848\n",
                        "conformer encoder, details={'num_blocks': 12, 'attention_dim': 512, 'attention_heads': 8}\n",
                        "melspec shape: torch.Size([1, 83, 80])\n",
                        "hidden_states shape: torch.Size([1, 5, 3072])\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import sys\n",
                "import torch\n",
                "import torchaudio\n",
                "import numpy as np\n",
                "\n",
                "# Mirror terminal export: PYTHONPATH=\"$(pwd)/fairseq:$(pwd):$PYTHONPATH\"\n",
                "repo_root = \"/ceph/home/TUG/olivares-tug/MMS-LLaMA\"\n",
                "fairseq_dir = os.path.join(repo_root, \"fairseq\")\n",
                "avhubert_dir = os.path.join(repo_root, \"avhubert\")\n",
                "os.environ[\"PYTHONPATH\"] = f\"{fairseq_dir}:{repo_root}:{avhubert_dir}:\" + os.environ.get(\"PYTHONPATH\", \"\")\n",
                "\n",
                "# Ensure repo + fairseq paths before importing fairseq\n",
                "src_dir = os.path.join(repo_root, \"src\")\n",
                "for p in [avhubert_dir, fairseq_dir, repo_root, src_dir]:\n",
                "    if p not in sys.path:\n",
                "        sys.path.insert(0, p)\n",
                "\n",
                "# Avoid AVHubert debug import path that triggers duplicate model registration\n",
                "if len(sys.argv) == 1:\n",
                "    sys.argv.append(\"run\")\n",
                "\n",
                "from fairseq import checkpoint_utils, tasks\n",
                "from fairseq.data.dictionary import Dictionary\n",
                "from transformers import AutoTokenizer, WhisperProcessor\n",
                "from omegaconf import OmegaConf\n",
                "\n",
                "import src.task\n",
                "from src.modelSpeech import MMS_LLaMA_Speech\n",
                "from src.utils import Compose, Normalize, CenterCrop, load_video\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "ckpt_path = \"/ceph/home/TUG/olivares-tug/MMS-LLaMA/pretrained_models/mms_llama/1759h/ckpt-1759h.pt\"\n",
                "llm_path = \"meta-llama/Llama-3.2-3B\"\n",
                "w2v_path = os.path.join(repo_root, \"pretrained_models/avhubert/muavic_multilingual_compatible.pt\")\n",
                "\n",
                "# Load config/task like demo.py (uses checkpoint defaults)\n",
                "model_overrides = {\n",
                "    \"task\": {\n",
                "        \"data\": os.path.join(repo_root, \"manifest/germanManifest\"),\n",
                "        \"label_dir\": os.path.join(repo_root, \"manifest/germanManifest\"),\n",
                "        \"llm_path\": llm_path,\n",
                "        \"noise_prob\": 0.75,\n",
                "        \"noise_wav\": os.path.join(repo_root, \"noise/babble_noise.wav\"),\n",
                "        \"normalize\": True,\n",
                "    },\n",
                "    \"model\": {\n",
                "        \"data\": os.path.join(repo_root, \"manifest/germanManifest\"),\n",
                "        \"w2v_path\": w2v_path,\n",
                "        \"llm_path\": llm_path,\n",
                "        \"dropout_input\": 0.0,\n",
                "        \"w2v_args\": None,\n",
                "        \"normalize\": True,\n",
                "        \"no_pretrained_weights\": False,\n",
                "        \"window_level\": False,\n",
                "        \"apply_mask\": False,\n",
                "        \"mask_selection\": \"static\",\n",
                "        \"mask_length\": 10,\n",
                "        \"mask_other\": 0,\n",
                "        \"mask_prob\": 0.75,\n",
                "        \"no_mask_overlap\": False,\n",
                "        \"mask_channel_selection\": \"static\",\n",
                "        \"mask_channel_length\": 64,\n",
                "        \"mask_channel_other\": 0,\n",
                "        \"mask_channel_prob\": 0.5,\n",
                "        \"no_mask_channel_overlap\": False,\n",
                "        \"layerdrop\": 0.1,\n",
                "        \"activation_dropout\": 0.1,\n",
                "        \"attention_dropout\": 0.0,\n",
                "        \"dropout\": 0.0,\n",
                "        \"feature_grad_mult\": 1.0,\n",
                "        \"freeze_finetune_updates\": 0,\n",
                "        \"sr_predictor_layers\": 2,\n",
                "        \"qformer_layers\": 2,\n",
                "        \"qformer_dim\": 1024,\n",
                "        \"queries_per_sec\": 3,\n",
                "        \"use_qformer\": True,\n",
                "        \"use_sr_predictor\": True,\n",
                "        \"whisper_embed_dim\": 1024,\n",
                "        \"avhubert_embed_dim\": 1024,\n",
                "        \"llama_embed_dim\": 3072,\n",
                "        \"modality_fuse\": \"concat\",\n",
                "        \"lora_rank\": 16,\n",
                "        \"lora_alpha\": 32,\n",
                "        \"target_modules\": \"q_proj.k_proj.v_proj.o_proj\",\n",
                "    },\n",
                "    \"common\": {\n",
                "        \"user_dir\": src_dir,\n",
                "    },\n",
                "}\n",
                "\n",
                "_, cfg, task = checkpoint_utils.load_model_ensemble_and_task(\n",
                "    [ckpt_path],\n",
                "    arg_overrides=model_overrides,\n",
                "    strict=False,\n",
                ")\n",
                "\n",
                "# Merge with training defaults to populate missing config keys\n",
                "base_cfg = OmegaConf.load(os.path.join(repo_root, \"src/conf/mms-llama.yaml\"))\n",
                "base_cfg.task.data = model_overrides[\"task\"][\"data\"]\n",
                "base_cfg.task.label_dir = model_overrides[\"task\"][\"label_dir\"]\n",
                "base_cfg.task.llm_path = model_overrides[\"task\"][\"llm_path\"]\n",
                "base_cfg.task.noise_prob = model_overrides[\"task\"][\"noise_prob\"]\n",
                "base_cfg.task.noise_wav = model_overrides[\"task\"][\"noise_wav\"]\n",
                "base_cfg.common.user_dir = src_dir\n",
                "base_cfg.model.w2v_path = w2v_path\n",
                "base_cfg.model.llm_path = llm_path\n",
                "\n",
                "cfg.task = OmegaConf.merge(base_cfg.task, cfg.task)\n",
                "cfg.model = OmegaConf.merge(base_cfg.model, cfg.model)\n",
                "cfg.common = OmegaConf.merge(base_cfg.common, cfg.common)\n",
                "\n",
                "# Build MMS_LLaMA_Speech from checkpoint config\n",
                "model = MMS_LLaMA_Speech.build_model(cfg.model, task)\n",
                "with torch.serialization.safe_globals([Dictionary]):\n",
                "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
                "model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
                "model.eval().to(device)\n",
                "\n",
                "# Prepare processors\n",
                "tokenizer = AutoTokenizer.from_pretrained(llm_path)\n",
                "whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium.en\")\n",
                "video_transform = Compose([\n",
                "    Normalize(0.0, 255.0),\n",
                "    CenterCrop((88, 88)),\n",
                "    Normalize(0.0, 1.0),\n",
                "])\n",
                "\n",
                "audio_path = \"/ceph/home/TUG/olivares-tug/datasets/lrs3/lrs3_video_seg24s/test/0Fi83BHQsMA/00002.wav\"\n",
                "video_path = \"/ceph/home/TUG/olivares-tug/datasets/lrs3/lrs3_video_seg24s/test/0Fi83BHQsMA/00002.mp4\"\n",
                "\n",
                "# Load raw audio and compute sample length (pre-Whisper padding)\n",
                "wav, sr = torchaudio.load(audio_path)\n",
                "if sr != 16000:\n",
                "    wav = torchaudio.transforms.Resample(sr, 16000)(wav)\n",
                "wav = wav.squeeze(0)\n",
                "audio_len_samples = torch.tensor([wav.numel()], dtype=torch.long, device=device)\n",
                "\n",
                "# Whisper input features\n",
                "audio_features = whisper_processor(wav.cpu().numpy(), sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
                "\n",
                "# Video features\n",
                "frames = load_video(video_path)\n",
                "frames = video_transform(frames)\n",
                "frames = np.expand_dims(frames, axis=-1)\n",
                "video_tensor = torch.from_numpy(frames.astype(np.float32))\n",
                "video_tensor = video_tensor.permute(3, 0, 1, 2).unsqueeze(0).to(device)\n",
                "\n",
                "# Padding mask for video frames\n",
                "T = video_tensor.shape[2]\n",
                "padding_mask = torch.zeros((1, T), dtype=torch.bool, device=device)\n",
                "\n",
                "# Instruction tokens (empty)\n",
                "instruction_tokens = tokenizer(\"\", return_tensors=\"pt\").input_ids[0].to(device)\n",
                "\n",
                "# Minimal empty labels list for forward_speech\n",
                "target_list = [torch.tensor([], dtype=torch.long, device=device)]\n",
                "\n",
                "with torch.no_grad():\n",
                "    out = model.forward_speech(\n",
                "        source={\n",
                "            \"audio\": audio_features,\n",
                "            \"video\": video_tensor,\n",
                "            \"instruction\": [instruction_tokens],\n",
                "            \"audio_lengths\": audio_len_samples,\n",
                "        },\n",
                "        padding_mask=padding_mask,\n",
                "        target_list=target_list,\n",
                "    )\n",
                "\n",
                "print(\"melspec shape:\", out[\"melspec\"].shape)\n",
                "print(\"hidden_states shape:\", out[\"hidden_states\"].shape)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
